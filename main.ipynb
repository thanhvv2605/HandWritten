{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '7', '109', '164', '255', '141', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '82', '221', '255', '255', '255', '223', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '77', '237', '255', '255', '255', '255', '223', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '146', '241', '255', '255', '255', '255', '255', '223', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '143', '255', '255', '255', '187', '128', '246', '255', '223', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '18', '91', '239', '255', '255', '173', '14', '128', '255', '255', '223', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '84', '255', '255', '255', '96', '18', '0', '109', '253', '255', '223', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '55', '235', '255', '255', '109', '14', '0', '0', '0', '239', '255', '223', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '239', '255', '255', '112', '0', '0', '0', '0', '0', '239', '255', '223', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '55', '164', '219', '253', '255', '187', '16', '0', '0', '0', '0', '0', '239', '255', '232', '32', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '191', '255', '255', '255', '255', '105', '36', '0', '0', '0', '0', '0', '219', '255', '255', '112', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '191', '255', '255', '255', '255', '255', '255', '146', '146', '146', '146', '146', '187', '255', '255', '112', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '80', '255', '255', '255', '255', '255', '255', '255', '255', '255', '255', '255', '255', '255', '255', '255', '173', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '121', '255', '255', '255', '228', '200', '255', '255', '255', '255', '255', '255', '255', '255', '255', '255', '255', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '16', '228', '255', '255', '237', '55', '18', '73', '73', '73', '73', '73', '73', '73', '141', '255', '255', '255', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '194', '255', '255', '255', '128', '0', '0', '0', '0', '0', '0', '0', '0', '0', '96', '255', '255', '255', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '255', '255', '255', '178', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '96', '255', '255', '255', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '214', '255', '241', '68', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '219', '255', '255', '132', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '32', '73', '59', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '137', '255', '255', '112', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '41', '228', '237', '48', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import các thư viện cần thiết\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# đọc dữ liệu từ file CSV\n",
    "with open('hand_written.csv', 'r') as csv_file:\n",
    "    result = csv.reader(csv_file)\n",
    "    rows = []\n",
    "    \n",
    "    # đọc từng dòng của file và thêm vào list rows, mỗi phần tử của list là một dòng\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "        \n",
    "# chọn một dòng từ dữ liệu, ở đây là dòng thứ 30000\n",
    "letter = rows[2000]\n",
    "\n",
    "# chuyển các giá trị từ string sang int và bỏ qua phần tử đầu tiên (có thể là nhãn)\n",
    "x = np.array([int(j) for j in letter[1:]])\n",
    "\n",
    "# chuyển đổi dữ liệu thành mảng 28x28\n",
    "x = x.reshape(28, 28)\n",
    "\n",
    "# in ra dòng đã chọn để kiểm tra\n",
    "print(letter)\n",
    "\n",
    "# hiển thị hình ảnh\n",
    "plt.imshow(x, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# train_data = [] # dữ liệu training\n",
    "# train_label = [] # label của chúng\n",
    "# i = 0\n",
    "# for letter in rows:\n",
    "#     if i < 10000:\n",
    "#         x = np.array([int(j) for j in letter[1:]])\n",
    "#         x = x.reshape(28, 28)\n",
    "#         train_data.append(x)\n",
    "#         train_label.append(int(letter[0]))\n",
    "#         i = i+1\n",
    "\n",
    "# print(len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "shuffle_order = list(range(len(train_label)))\n",
    "random.shuffle(shuffle_order)\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "train_label = np.array(train_label)\n",
    "\n",
    "train_data = train_data[shuffle_order]\n",
    "train_label = train_label[shuffle_order]\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Định nghĩa mô hình mạng nơ-ron tích chập\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Lớp tích chập đầu tiên\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Lớp tích chập thứ hai\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Lớp fully connected (lớp kết nối đầy đủ)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Giả sử input image size là 28x28\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 lớp output (cho 10 class)\n",
    "\n",
    "        # Lớp dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Lớp tích chập 1 -> ReLU -> MaxPool\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        \n",
    "        # Lớp tích chập 2 -> ReLU -> MaxPool\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flattening (biến đổi dữ liệu thành 1 chiều để đưa vào fully connected)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        \n",
    "        # Fully connected layer 1 với dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        \n",
    "        # Fully connected layer 2 (output)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "model = ConvNet()\n",
    "\n",
    "# Định nghĩa loss function và optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function cho phân loại đa lớp\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Chuyển đổi nhãn thành one-hot encoding trong PyTorch sử dụng nn.functional\n",
    "def to_categorical(y, num_classes):\n",
    "    return F.one_hot(y, num_classes)\n",
    "\n",
    "# Giả sử input là batch hình ảnh 28x28\n",
    "input_data = torch.randn(32, 1, 28, 28)  # Batch size 32, 1 channel, kích thước 28x28\n",
    "output = model(input_data)\n",
    "\n",
    "print(output.shape)  # Kiểm tra output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Giả sử train_data và train_label đã được tải hoặc xử lý trước\n",
    "# Ví dụ: train_data = np.load('train_data.npy'), train_label = np.load('train_label.npy')\n",
    "\n",
    "# Chia dữ liệu thành tập huấn luyện và tập test (80% train, 20% test)\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_data, train_label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Chia thêm tập huấn luyện thành tập huấn luyện và tập xác thực (validation)\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Các tham số mô hình\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 28\n",
    "N_CLASSES = 4\n",
    "LR = 0.001\n",
    "N_EPOCHS = 50\n",
    "\n",
    "# Định nghĩa mô hình CNN\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Lớp tích chập 1 (32 filters, kernel size 3x3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # Lớp tích chập 2 (64 filters, kernel size 3x3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        # Lớp tích chập 3\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Lớp max pooling (kích thước nhỏ hơn để tránh thu nhỏ quá nhanh)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Lớp fully connected (kết nối đầy đủ)\n",
    "        self.fc1 = nn.Linear(32 * 3 * 3, 1024)  # Đã điều chỉnh kích thước\n",
    "        self.fc2 = nn.Linear(1024, N_CLASSES)  # 4 lớp output (cho 4 class)\n",
    "        \n",
    "        # Lớp dropout\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv1 -> ReLU -> MaxPool\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        \n",
    "        # Conv2 -> ReLU -> MaxPool\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Conv3 -> ReLU -> MaxPool (giảm bớt số lớp pooling)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flattening\n",
    "        x = x.view(-1, 32 * 3 * 3)  # Kích thước điều chỉnh sau pooling\n",
    "        \n",
    "        # Fully connected layer 1 với dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        \n",
    "        # Fully connected layer 2 (output layer)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "model = ConvNet()\n",
    "\n",
    "# Định nghĩa loss function và optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Giả sử input là batch hình ảnh 28x28\n",
    "input_data = torch.randn(BATCH_SIZE, 1, IMG_SIZE, IMG_SIZE)  # Batch size 32, 1 channel, kích thước 28x28\n",
    "output = model(input_data)\n",
    "\n",
    "print(output.shape)  # Kiểm tra kích thước output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "val_x = val_x.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "test_x = test_x.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Chuyển đổi numpy array thành tensor trước khi sử dụng F.one_hot\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "val_y = torch.tensor(val_y, dtype=torch.long)\n",
    "test_y = torch.tensor(test_y, dtype=torch.long)\n",
    "\n",
    "# Sử dụng F.one_hot để chuyển thành one-hot encoding\n",
    "train_y = F.one_hot(train_y, N_CLASSES)\n",
    "val_y = F.one_hot(val_y, N_CLASSES)\n",
    "test_y = F.one_hot(test_y, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w5/txck4bg523d_v2l_9zf_wrzw0000gn/T/ipykernel_8561/2923646694.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_y = torch.tensor(train_y, dtype=torch.long)  # Đối với nhãn, dùng dtype long cho bài toán phân loại\n",
      "/var/folders/w5/txck4bg523d_v2l_9zf_wrzw0000gn/T/ipykernel_8561/2923646694.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_y = torch.tensor(val_y, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Nếu train_x và train_y là numpy arrays, hãy chuyển đổi chúng sang Tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)  # Đối với nhãn, dùng dtype long cho bài toán phân loại\n",
    "\n",
    "val_x = torch.tensor(val_x, dtype=torch.float32)\n",
    "val_y = torch.tensor(val_y, dtype=torch.long)\n",
    "# Tạo DataLoader sau khi chắc chắn rằng train_x và train_y là tensor và có cùng số mẫu\n",
    "train_data = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_data = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Đặt lại gradient\u001b[39;00m\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tính loss\u001b[39;00m\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Cập nhật trọng số\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "# Giả sử bạn đã có các biến: train_x, train_y, val_x, val_y dưới dạng PyTorch Tensor\n",
    "for inputs, labels in train_loader:\n",
    "    # Thay đổi thứ tự từ NHWC (batch_size, height, width, channels) sang NCHW (batch_size, channels, height, width)\n",
    "    inputs = inputs.permute(0, 3, 1, 2)  # Từ (B, H, W, C) -> (B, C, H, W)\n",
    "    \n",
    "    optimizer.zero_grad()  # Đặt lại gradient\n",
    "    outputs = model(inputs)  # Forward pass\n",
    "    loss = criterion(outputs, labels)  # Tính loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Cập nhật trọng số\n",
    "# Các tham số cần thiết\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "\n",
    "# Định nghĩa loss function và optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Sử dụng CrossEntropy cho bài toán phân loại\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)  # Optimizer Adam\n",
    "\n",
    "# Tạo DataLoader để chia dữ liệu thành các batch\n",
    "train_data = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_data = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Vòng lặp huấn luyện\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()  # Chuyển mô hình sang chế độ huấn luyện\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Đặt lại gradient\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Tính loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Cập nhật trọng số\n",
    "        \n",
    "        # Tính toán các chỉ số\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # In ra loss và accuracy trên tập huấn luyện\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}, Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Đánh giá trên tập validation\n",
    "    model.eval()  # Chuyển mô hình sang chế độ đánh giá\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Tắt tính toán gradient cho tập validation\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "            loss = criterion(val_outputs, val_labels)\n",
    "            val_loss += loss.item()\n",
    "            _, val_predicted = torch.max(val_outputs, 1)\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu các tham số của mô hình (state_dict)\n",
    "torch.save(model.state_dict(), 'mymodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo lại kiến trúc mô hình\n",
    "model = ConvNet()  # Phải giống với kiến trúc đã lưu\n",
    "\n",
    "# Tải tham số vào mô hình\n",
    "model.load_state_dict(torch.load('mymodel.pth'))\n",
    "# Chuyển mô hình sang chế độ đánh giá (không cần huấn luyện nữa)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dự đoán với tập dữ liệu test\n",
    "test_logits = model.predict(test_x)\n",
    "#lấy phần tử có giá trị lớn nhất \n",
    "test_logits = np.argmax(test_logits, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(test_logits == original_test_y) / len(test_logits))\n",
    "#result: 0.9964297306069458\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
